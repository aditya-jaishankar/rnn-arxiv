{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import itertools\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to be able to extract all the data. For this we use `BeautifulSoup` to extract all the text between the `<summary>` and `</summary>` tags. We then have a list of all abstracts. We can then look at it from a character level perspective, train an LSTM (say we use Karpathy's architecture instead of fooling around with different architectures) and then have it output an abstract of say 500 words. This should be an interesting exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the corpus and extracting the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadDataCorpus(parsehtml=False, downloadfile=False):\n",
    "    \"\"\"\n",
    "    Creates the corpus.\n",
    "    Inputs:\n",
    "        parsehtml: whether to run this function at all. I should normally have this\n",
    "        corpus presaved as a text file\n",
    "        downloadfile: whether to download the data or not. This seems to be the speed\n",
    "        bottleneck. \n",
    "    \"\"\"\n",
    "    if not parsehtml:\n",
    "        return None\n",
    "    \n",
    "    if downloadfile:\n",
    "        url = 'http://export.arxiv.org/api/query?search_query=all:polymer&start=0&max_results=10000'\n",
    "        f = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        with open(r'./rawData.txt', 'w+', encoding='utf-8') as out:\n",
    "            out.write(str(soup))\n",
    "    soup = BeautifulSoup(open('./rawData.txt', encoding='utf-8'), 'html.parser')\n",
    "    abstracts = []\n",
    "    for summary in tqdm(soup.find_all('summary')):\n",
    "        abstracts.append(summary.text.replace('\\n', ' ').strip())\n",
    "    abstracts = np.array(abstracts)\n",
    "    np.save('./abstracts.npy', abstracts)\n",
    "\n",
    "downloadDataCorpus(parsehtml=False, downloadfile=False)\n",
    "\n",
    "corpus = np.load('./abstracts.npy')\n",
    "all_text = ''.join(corpus)  # combine all abstracts into a single string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining some utility functions\n",
    "\n",
    "### Encoding the characters into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "def encoder(string):\n",
    "    \"\"\"\n",
    "    This function takes a string and tokenizes it by assigning it a unique index\n",
    "    \"\"\"\n",
    "    encoded = torch.zeros(len(string)).long()\n",
    "    for i, char in enumerate(string):\n",
    "        encoded[i] = all_characters.index(char)\n",
    "    return encoded\n",
    "\n",
    "def save():\n",
    "    \"\"\"\n",
    "    To save any intermediate models while training in case I give a\n",
    "    KeyboardInterrupt.\n",
    "    \"\"\"\n",
    "    torch.save(lstm_model.state_dict(), './arxiv-generator2.pt')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a `Dataset` object to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainingSet(Dataset):\n",
    "\n",
    "    def __init__(self, all_text, str_length):\n",
    "        super(trainingSet, self).__init__()\n",
    "        self.all_text = all_text\n",
    "        self.str_length = str_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seed = np.random.randint(0, len(self.all_text) - self.str_length)\n",
    "        sequence = self.all_text[seed:seed+str_length]\n",
    "        inp = encoder(sequence[:-1])\n",
    "        target = encoder(sequence[1:])\n",
    "        return inp, target\n",
    "\n",
    "str_length = 25\n",
    "batch_size = 100\n",
    "dataset = trainingSet(all_text, str_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the structure of the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next need to define my lstm class. The need to define the training loop. Then need to define an abstract generator loop. if count('.') > 10, then break (this would keep the abstract a reasonable length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = INPUT_SIZE\n",
    "        self.hidden_size = HIDDEN_SIZE\n",
    "        self.output_size = OUTPUT_SIZE\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(INPUT_SIZE, HIDDEN_SIZE)\n",
    "        self.lstm = nn.LSTM(HIDDEN_SIZE, HIDDEN_SIZE, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        batch_size = inp.size(0) # Think about what exactly is meant by batch_size here\n",
    "        encoded = self.encoder(inp)\n",
    "        output, hidden = self.lstm(encoded.view(1,batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "    \n",
    "    # Probably lets me handle predictions very nicely\n",
    "    def forward2(self, inp, hidden):\n",
    "        encoded = self.encoder(inp.view(1, -1))\n",
    "        output, hidden = self.lstm(encoded.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = lstm_model.initHidden(batch_size)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(str_length-1):\n",
    "        output, hidden = lstm_model(inp[:, i], hidden)\n",
    "        loss += loss_fn(output.view(batch_size, -1), target[:, i])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()/str_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (encoder): Embedding(100, 128)\n",
       "  (lstm): LSTM(128, 128, num_layers=2)\n",
       "  (decoder): Linear(in_features=128, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 1000000\n",
    "INPUT_SIZE = n_characters\n",
    "HIDDEN_SIZE = 128\n",
    "OUTPUT_SIZE = n_characters\n",
    "num_layers = 2\n",
    "lstm_model = LSTM(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, num_layers=num_layers)\n",
    "\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "try:\n",
    "    for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "        inp, target = next(iter(dataloader))\n",
    "        loss = train(inp.to(device), target.to(device))\n",
    "        loss_avg += loss\n",
    "\n",
    "        if (epoch + 1) % 10000 == 0:\n",
    "            print('loss so far: ', loss)\n",
    "            all_losses.append(loss_avg)\n",
    "            loss_avg = 0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "        print('Saving model...')\n",
    "        save()\n",
    "\n",
    "save()\n",
    "plt.plot(range(len(all_losses)), all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to generate the text\n",
    "\n",
    "Once the model has been trained, we can use it to generate text. Note that instead of using the character expected from the argmax of the highest probability, we actually sample from a multinomial distribution based on an 'activation' temperature. Higher the value of temperature, more noise we would expect to see, and a larger number os spelling and other errors. However, very low temperatures would give very little diversity and we end up generating text in a deterministic manner given the priming string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here we study the self-avoiding walks leads to the semi-flexibon. This behavior with enhanced as displacement is influenced by the microstrating Fermi statistical models as the large size in large attractive model based on a potential formalism. The model is explored theory is defined as the T$_{0}$ concentrational statistics to show that the polymers in DNA structure can be extended to the mechanical jump above the constraint in turn on a polymer network and simulations where our approach to the beam constant exists a consecutive circuits of the electrically to unfold in polymer chains and electr'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(decoder, prime_str='Here', predict_len=600, temperature=0.8, cuda=False):\n",
    "    hidden_state, cell_state = decoder.initHidden(1)\n",
    "    hidden = hidden_state.cpu(), cell_state.cpu()\n",
    "    prime_input = encoder(prime_str).unsqueeze(0)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[:,p], hidden)\n",
    "        \n",
    "    inp = prime_input[:,-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = encoder(predicted_char).unsqueeze(0)\n",
    "\n",
    "    return predicted\n",
    "\n",
    "lstm_model_pred = LSTM(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, num_layers=num_layers).cpu()\n",
    "\n",
    "lstm_model_pred.load_state_dict(torch.load('./arxiv-generator2.pt', map_location=torch.device('cpu')))\n",
    "lstm_model_pred.eval()\n",
    "generate(lstm_model_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the abstract makes very good grammatical and lexical sense, and nearly reads like an accepted abstract!"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
